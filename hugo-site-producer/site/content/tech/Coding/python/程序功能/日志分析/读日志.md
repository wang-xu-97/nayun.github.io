---
title: "读日志"
date: 2025-07-30T16:44:17+08:00
draft: false
tags: ["技", "code", "python", '日志', '文件处理']
toc: true
showTableOfContents: true
---

## 读取单个日志
### 1. 读全部日志
处理方法：直接读取整体文件内容，或者逐行读取，返回lines列表
```python
def readlog(logfile):
    with open(logfile, 'r') as f:
        return f.read() or f.readlines()
```
### 2. 读部分日志（限定时间范围）
需要考虑：日志文件过大，逐行判断时间戳性能过低
处理方法：
1. 二分查找起始位置
```python
import datetime
import os

def bs_read_log(logfile, st, et):
    text = []
    file_size = os.path.getsize(logfile)
    
    with open(logfile, 'r', encoding='utf-8') as f:
        # 二分查找定位起始位置
        low, high = 0, file_size
        start_pos = 0
        
        while low < high:
            mid = (low + high) // 2
            f.seek(mid)
            f.readline()  # 移动到完整行首
            line = f.readline()
            
            if not line:
                break
                
            line_tm = logline_time(line)
            if line_tm is None:  # 跳过无效行
                continue
                
            if line_tm < st:
                low = mid + 1
                start_pos = f.tell()  # 记录最后有效位置
            else:
                high = mid
        
        # 定位到起始区域
        f.seek(start_pos)
        while True:
            line = f.readline()
            if not line:
                break
                
            line_tm = logline_time(line)
            if line_tm is None:
                continue
                
            if line_tm > et:
                break
                
            if st <= line_tm <= et:
                text.append(line.strip())
    
    return text
```
2. 二分查找起始位置 + 分块检测日志时间
```python
def readlog(logfile, st, et):
    # 入参检测
    if st > et:
        return None
    # 根据日志大小设置分块大小
    chunksize = 1024
    # chunk值初始设置为chunksize 大小，可以在读第一行日志时先判断日志时间是否在(st, et)区间，不在即可直接退出
    chunk = 1024
    text = []
    file_size = os.path.getsize(logfile)
    with open(logfile, 'rb') as f:
        # 二分查找定位起始位置
        low, high = 0, file_size
        start_pos = 0
        
        while low < high:
            mid = (low + high) // 2
            f.seek(mid)
            f.readline()  # 移动到完整行首
            line = f.readline()
            
            if not line:
                break
                
            line_tm = logline_time(line)
            if line_tm is None:  # 跳过无效行
                continue
                
            if line_tm < st:
                low = mid + 1
                start_pos = f.tell()  # 记录最后有效位置
            else:
                high = mid

        # 定位到起始区域
        f.seek(start_pos)
        while(line:=f.readline().decode(errors='ignore')):
            chunk += 1
            if chunk >= chunksize:
                chunk = 1
                if (tm:=logline_time(line)) != None and tm > et:
                    break

            text.append(line)

    # 如果追求读取范围结束时间准确性，可以从后往前逐行清除区间外日志，不追求结束时间准确性可以不做
    while logline_time(text[-1]) > et:
        text.pop(-1)
    return text
```
### 3. 反向读日志
思路：定位到文件尾，逐行读取
核心：
1. 定位文件尾：
    `file.seek(0, 2)`
2. 逐个字符读取，直到换行符yield输出：
    1. `buffer = file.read(1) + buffer` 需要捕获处理file.read的异常
    2. `if buffer[0] == '\n':yield buffer`
```python
def r_read(fp):
    with open(fp, 'rb') as f:
        f.seek(0, 2)
        pos = f.tell()
        buffer = ''
        while pos >= 0:
            f.seek(pos)
            try:
                c = f.read(1)
            except:
                print('decode error, pass')
                c = ''
            if c == '\n':
                yield buffer
                buffer = ''
            else:
                buffer = c + buffer
            pos -= 1
        # 边界情况
        yield buffer

for line in r_read():
    # 满足条件退出
    if condition(line):
        break
else:
    # 未找到符合条件的日志
    print(f'NA')
```

## 批量读取日志
1. 需求：从一批log文件中，读取指定时间范围的日志内容
1. 思路：
    1. Path捕获全部符合指定文件名模式的日志
    2. 通过日志名中的时间命名规则，过滤时间范围外的日志文件
    3. 排序，逐个读取单个日志文件，日志整合

```python
def batch_read_log(logpath, timerange):
    text = []
    logpattern = f"*.log"
    get_log_time = lambda log:timestr2ts(log.stem.split('-')[-1].split('.')[0], '%Y_%m_%d_%H_%M_%S')
    logfiles = sorted(Path(logpath).rglob(logpattern), key=lambda p:p.stem)
    lognum = len(logfiles)
    logger.info(f'Reading logs (total {lognum})...')
    i = 0
    for i, log in enumerate(logfiles):
        if get_log_time(log) >= timerange[0]:
            break
        
    readlogs = []
    while i < lognum and (logtext:=readlog(logfiles[i], timerange[0], timerange[1])):
        if logfiles[i].name in readlogs:
            i += 1
            continue
        logger.debug(f'reading {logfiles[i].name}')
        text.extend(logtext)
        readlogs.append(logfiles[i].name)
        i += 1

    size = len(text)
    if size == 0:
        logger.warning('read 0 logs')
        return text
    logbegin = logline_time(text[0])[1]
    while (logend:=logline_time(text[-1])) == -1:
        text.pop(-1)
    logger.info(f'read {len(readlogs):>2} logs [ {logbegin[:19]} ~ {logend[1][:19]} ]  size:{size}')
    return text
```