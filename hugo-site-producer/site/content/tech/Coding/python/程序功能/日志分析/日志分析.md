---
title: "日志分析"
date: 2025-07-30T16:44:17+08:00
draft: false
tags: ["技", "code", "python", '日志', '数据处理', 'linux', '闭包']
toc: true
showTableOfContents: true
---
# 纯python处理
`支持关键字正则表达式组合，逐行后处理，跨平台兼容`
## 日志关键字处理
需求：
1. 1次遍历，O(n)
2. 多关键字(支持regex pattern)
3. 关键字一对一自定义处理方法解耦
4. 处理结果统计，每个关键字的成功/失败处理量+处理总量

例子：
找到日志中包含kwA的日志，执行dealA(line) -> pass/fail处理方法
找到日志中包含kwB的日志，执行dealB(line) -> pass/fail处理方法
...

思路：
1. 每行日志查询全部关键字，有关键字出现，进入deallog回调
1. 通过deal_log回调函数，在回调内部区分kw，执行不同逻辑
1. 结束时输出关键字相关处理总量

```python
import re
from collection import defaultdict
def keywords_check(checktitle, text, checklist, deal_log=(lambda pt, line:True)) -> int, str:
    # checklist 入参多类型支持(dict, list)
    checklist = list(dict.fromkeys(checklist))
    # 计数器
    kw_counter, err_counter = defaultdict(int), defaultdict(int)
    for line in text:
        found = [pattern for pattern in checklist if re.search(pattern, line) is not None]
        if not found:continue
        for pattern in found:
            kw_counter[pattern] += 1
            if False == deal_log(pattern, line):
                err_counter[pattern] += 1
    msg = "{} done, total {} records, {} errors\n{}".format(
        checktitle
        , (kw_sum:=sum(kw_counter.values()))
        , sum(err_counter.values())
        , "".join(["[{}]: {} records, {} errors\n".format(
            pattern
            , kw_counter.get(pattern, 0)
            , err_counter.get(pattern, 0)
            ) for pattern in checklist
        ])
    )
    return kw_sum, msg

checklist = {
    'pt1':r"^kwA*$",
    'pt2':r"^kwB*$",
    'pt3':r"^kwC*$",
}
def deal_log(pt, line):
    if pt == checklist['pt1']:
        return dealA(line)
    elif pt == checklist['pt2']:
        return dealB(line)
    elif pt == checklist['pt3']:
        return dealC(line)
    else:
        return False

keywords_check('check keywords', checklist.values(), deal_log)
```

## 数据收集
可以使用上面的`keywords_check`方法，通过自定义deal_log方法收集数据
完成数据收集之后自定义数据分析方法
```python
collection = defaultdict(list)
collect_pts = {
    'pt1':collect_pt1,
    'pt2':collect_pt2,
    'pt3':collect_pt3,
}
def deal_log(pt, line):
    if pt == checklist['pt1']
        match = re.findall(collect_pts['pt1'], line)
        collection[pt].append(match)
        # 每个匹配处理完成后一定要return，不然可能重复处理
        return
    if pt == checklist['pt2']
        match = re.findall(collect_pts['pt2'], line)
        collection[pt].append(match)
        # 每个匹配处理完成后一定要return，不然可能重复处理
        return
    if pt == checklist['pt3']
        match = re.findall(collect_pts['pt3'], line)
        collection[pt].append(match)
        # 每个匹配处理完成后一定要return，不然可能重复处理
        return
```
因为每个关键字获取的模式一致，数据列一致，所以可以使用pandas Dataframe快速生成dataframe数据对象
```python
import pandas as pd
df = pd.Dataframe(collection['pt1'], columns)
```
# 使用linux grep管道
优点：
- 极高性能：grep 用 C 编写，高度优化（如 Boyer-Moore 算法）。
- 并行处理：部分 grep 实现支持多线程（如 GNU grep 的 --parallel）。
- 管道优化：内核级 I/O 缓冲减少磁盘访问次数。

缺点：
- 不支持正则表达式组合
- 不支持逐行后处理
```python
import subprocess
subprocess.run(["grep", "-F", "keyword", "large.log"])
```
# todo：Python mmap 优化：

```python
import mmap
with open('large.log', 'r+b') as f:
    mm = mmap.mmap(f.fileno(), 0)
    for line in iter(mm.readline, b""):
        if b'keyword' in line:
            process(line)
```